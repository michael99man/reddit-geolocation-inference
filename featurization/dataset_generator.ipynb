{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates training and test datasets (ChungusSets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'featurization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-862fb01272a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeaturization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'featurization'"
     ]
    }
   ],
   "source": [
    "\"\"\" Computes the full feature vectors based on the time_model, word2vec model, and subreddits\n",
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from word2vec import EpochSaver\n",
    "import pymysql\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from featurization import featurizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "model = KeyedVectors.load('../models/full.model')\n",
    "word = 'bob'\n",
    "print(word in model.wv.vocab)\n",
    "print(model.wv.vocab.get(word).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"Alaska\", \"Alabama\", \"Arkansas\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:37:45: loading Word2VecKeyedVectors object from ../models/full.model\n",
      "INFO - 09:37:46: loading wv recursively from ../models/full.model.wv.* with mmap=None\n",
      "INFO - 09:37:46: loading vectors from ../models/full.model.wv.vectors.npy with mmap=None\n",
      "INFO - 09:37:46: setting ignored attribute vectors_norm to None\n",
      "INFO - 09:37:46: loading vocabulary recursively from ../models/full.model.vocabulary.* with mmap=None\n",
      "INFO - 09:37:46: loading trainables recursively from ../models/full.model.trainables.* with mmap=None\n",
      "INFO - 09:37:46: loading syn1neg from ../models/full.model.trainables.syn1neg.npy with mmap=None\n",
      "INFO - 09:37:46: setting ignored attribute cum_table to None\n",
      "INFO - 09:37:46: loaded ../models/full.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 entries\n",
      "Processed 2000 entries\n",
      "Processed 3000 entries\n",
      "Processed 4000 entries\n",
      "Processed 5000 entries\n",
      "Processed 6000 entries\n",
      "Processed 7000 entries\n",
      "Processed 8000 entries\n",
      "Processed 9000 entries\n",
      "Processed 10000 entries\n",
      "Processed 11000 entries\n",
      "Processed 12000 entries\n",
      "Processed 13000 entries\n",
      "Processed 14000 entries\n",
      "Processed 15000 entries\n",
      "Processed 16000 entries\n",
      "Processed 17000 entries\n",
      "Processed 18000 entries\n",
      "Processed 19000 entries\n",
      "Processed 20000 entries\n",
      "Processed 21000 entries\n",
      "Processed 22000 entries\n",
      "Processed 23000 entries\n",
      "Processed 24000 entries\n",
      "Processed 25000 entries\n",
      "Processed 26000 entries\n",
      "Processed 27000 entries\n",
      "Processed 28000 entries\n",
      "Processed 29000 entries\n",
      "Processed 30000 entries\n",
      "Processed 31000 entries\n",
      "Processed 32000 entries\n",
      "Processed 33000 entries\n",
      "Processed 34000 entries\n",
      "Processed 35000 entries\n",
      "Processed 36000 entries\n",
      "Processed 37000 entries\n",
      "Processed 38000 entries\n",
      "Processed 39000 entries\n",
      "Processed 40000 entries\n",
      "Processed 41000 entries\n",
      "Processed 42000 entries\n",
      "Processed 43000 entries\n",
      "Processed 44000 entries\n",
      "Processed 45000 entries\n",
      "Processed 46000 entries\n",
      "Processed 47000 entries\n",
      "Processed 48000 entries\n",
      "Processed 49000 entries\n",
      "Processed 50000 entries\n",
      "Processed 51000 entries\n",
      "Processed 52000 entries\n",
      "Processed 53000 entries\n",
      "Processed 54000 entries\n",
      "Processed 55000 entries\n",
      "Processed 56000 entries\n",
      "Processed 57000 entries\n",
      "Processed 58000 entries\n",
      "Processed 59000 entries\n",
      "Processed 60000 entries\n",
      "Processed 61000 entries\n",
      "Processed 62000 entries\n",
      "Processed 63000 entries\n",
      "Processed 64000 entries\n",
      "Processed 65000 entries\n",
      "Processed 66000 entries\n",
      "Processed 67000 entries\n",
      "Processed 68000 entries\n",
      "Processed 69000 entries\n",
      "Processed 70000 entries\n",
      "Processed 71000 entries\n"
     ]
    }
   ],
   "source": [
    "data = featurizer.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"data_dump.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORDS = 10000\n",
    "\n",
    "def data_to_dataset(data):\n",
    "    # lists that will eventually be turned into tensors and into the dataset\n",
    "    labels = []\n",
    "    words = []\n",
    "    subs = []\n",
    "    times = []\n",
    "    \n",
    "    count = 0\n",
    "    for (k, v) in data.items():\n",
    "        # these are all foreigners or too short\n",
    "        if('time_v' not in v or v['location'] not in states or len(v['document']) < N_WORDS):\n",
    "            continue\n",
    "        \n",
    "        labels.append(states.index(v['location']))\n",
    "            \n",
    "        words_tensor = torch.tensor(v['document'][:N_WORDS], dtype=torch.long).type(torch.LongTensor)\n",
    "        words.append(words_tensor)\n",
    "        \n",
    "        sub_tensor = torch.tensor(v['subreddit_v'], dtype=torch.float)\n",
    "        subs.append(sub_tensor)\n",
    "        \n",
    "        times_tensor = torch.tensor(v['time_v'], dtype=torch.float)\n",
    "        times.append(times_tensor)\n",
    "        \n",
    "        count += 1\n",
    "        if(count % 10000 == 0):\n",
    "            print(\"Processed %d entries\" % count)\n",
    "        \n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long).type(torch.LongTensor)\n",
    "    words = torch.stack(words, dim=0).type(torch.LongTensor)\n",
    "    subs = torch.stack(subs,dim=0)\n",
    "    times = torch.stack(times, dim=0)\n",
    "    \n",
    "    return featurizer.ChungusSet(words, subs, times, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 entries\n",
      "Processed 20000 entries\n",
      "Processed 30000 entries\n",
      "Processed 40000 entries\n",
      "Processed 50000 entries\n"
     ]
    }
   ],
   "source": [
    "dataset = data_to_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "N_TRAIN = int(4/5 * len(dataset))\n",
    "N_TEST = len(dataset) - N_TRAIN\n",
    "torch.manual_seed(0)\n",
    "\n",
    "datasets = torch.utils.data.random_split(dataset, [N_TRAIN, N_TEST])\n",
    "train_data = datasets[0]\n",
    "test_data = datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "pickle.dump(train_data, open(\"../data/train_set.p\", \"wb\"),  protocol=4)\n",
    "pickle.dump(test_data, open(\"../data/test_set.p\", \"wb\"),  protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
